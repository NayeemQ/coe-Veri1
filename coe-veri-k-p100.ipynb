{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930bb7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from sklearn.metrics import average_precision_score\n",
    "import math\n",
    "\n",
    "class VeRiDataset(Dataset):\n",
    "    def __init__(self, data_dir, split='train', transform=None, triplet_sampling=False):\n",
    "        \"\"\"\n",
    "        Initialize the VeRi dataset.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): Root directory of the VeRi dataset\n",
    "            split (str): 'train', 'query', or 'test'\n",
    "            transform: PyTorch transforms for image preprocessing\n",
    "            triplet_sampling (bool): Whether to enable triplet sampling for training\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.triplet_sampling = triplet_sampling\n",
    "        self.image_dir = {\n",
    "            'train': 'image_train',\n",
    "            'query': 'image_query',\n",
    "            'test': 'image_test'\n",
    "        }[split]\n",
    "        self.data = self._load_data()\n",
    "        self.vehicle_id_to_idx = {vid: idx for idx, vid in enumerate(sorted(set(item['vehicle_id'] for item in self.data)))}\n",
    "        self.camera_id_to_idx = {cid: idx for idx, cid in enumerate(sorted(set(item['camera_id'] for item in self.data)))}\n",
    "\n",
    "        if triplet_sampling and split == 'train':\n",
    "            self._build_triplet_index()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Load image file names and labels based on the split.\"\"\"\n",
    "        name_file = {\n",
    "            'train': 'name_train.txt',\n",
    "            'query': 'name_query.txt',\n",
    "            'test': 'name_test.txt'\n",
    "        }[self.split]\n",
    "\n",
    "        try:\n",
    "            with open(os.path.join(self.data_dir, name_file), 'r', encoding='utf-8') as f:\n",
    "                image_files = [line.strip().split()[0] for line in f.readlines() if line.strip()]\n",
    "        except UnicodeDecodeError:\n",
    "            with open(os.path.join(self.data_dir, name_file), 'r', encoding='latin-1') as f:\n",
    "                image_files = [line.strip().split()[0] for line in f.readlines() if line.strip()]\n",
    "\n",
    "        label_file = 'train_label.xml' if self.split == 'train' else 'test_label.xml'\n",
    "        label_path = os.path.join(self.data_dir, label_file)\n",
    "\n",
    "        label_dict = {}\n",
    "        try:\n",
    "            with open(label_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            root = ET.fromstring(content)\n",
    "        except (UnicodeDecodeError, ET.ParseError):\n",
    "            try:\n",
    "                with open(label_path, 'r', encoding='latin-1') as f:\n",
    "                    content = f.read()\n",
    "                root = ET.fromstring(content)\n",
    "            except ET.ParseError:\n",
    "                try:\n",
    "                    tree = ET.parse(label_path)\n",
    "                    root = tree.getroot()\n",
    "                except ValueError:\n",
    "                    with open(label_path, 'rb') as f:\n",
    "                        content = f.read().decode('utf-8', errors='ignore')\n",
    "                    root = ET.fromstring(content)\n",
    "\n",
    "        items_element = root.find('Items')\n",
    "        if items_element is not None:\n",
    "            for item in items_element.findall('Item'):\n",
    "                img_name = item.get('imageName')\n",
    "                if img_name:\n",
    "                    label_dict[img_name] = {\n",
    "                        'vehicle_id': item.get('vehicleID', '0'),\n",
    "                        'camera_id': item.get('cameraID', 'c001'),\n",
    "                        'color_id': item.get('colorID', '1'),\n",
    "                        'type_id': item.get('typeID', '1')\n",
    "                    }\n",
    "\n",
    "        data = []\n",
    "        for img_file in image_files:\n",
    "            if img_file in label_dict:\n",
    "                img_path = os.path.join(self.data_dir, self.image_dir, img_file)\n",
    "                if os.path.exists(img_path):\n",
    "                    data.append({\n",
    "                        'img_path': img_path,\n",
    "                        'vehicle_id': label_dict[img_file]['vehicle_id'],\n",
    "                        'camera_id': label_dict[img_file]['camera_id'],\n",
    "                        'color_id': label_dict[img_file]['color_id'],\n",
    "                        'type_id': label_dict[img_file]['type_id'],\n",
    "                        'img_name': img_file\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Warning: Image file not found: {img_path}\")\n",
    "            else:\n",
    "                print(f\"Warning: No label found for image: {img_file}\")\n",
    "\n",
    "        print(f\"Loaded {len(data)} samples for {self.split} split\")\n",
    "        return data\n",
    "\n",
    "    def _build_triplet_index(self):\n",
    "        \"\"\"Build index for efficient triplet sampling.\"\"\"\n",
    "        self.vehicle_to_images = defaultdict(list)\n",
    "        for idx, item in enumerate(self.data):\n",
    "            self.vehicle_to_images[item['vehicle_id']].append(idx)\n",
    "        self.vehicle_ids = list(self.vehicle_to_images.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.triplet_sampling and self.split == 'train':\n",
    "            return self._get_triplet(idx)\n",
    "        else:\n",
    "            return self._get_single_item(idx)\n",
    "\n",
    "    def _get_single_item(self, idx):\n",
    "        item = self.data[idx]\n",
    "        try:\n",
    "            img = Image.open(item['img_path']).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {item['img_path']}: {e}\")\n",
    "            img = Image.new('RGB', (224, 224), color=(0, 0, 0))\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        vehicle_idx = self.vehicle_id_to_idx[item['vehicle_id']]\n",
    "        camera_idx = self.camera_id_to_idx[item['camera_id']]\n",
    "\n",
    "        return {\n",
    "            'img': img,\n",
    "            'vehicle_id': vehicle_idx,\n",
    "            'camera_id_idx': camera_idx,\n",
    "            'img_name': item.get('img_name', None)\n",
    "        }\n",
    "\n",
    "    def _get_triplet(self, idx):\n",
    "        anchor_item = self.data[idx]\n",
    "        anchor_vehicle_id = anchor_item['vehicle_id']\n",
    "\n",
    "        pos_candidates = self.vehicle_to_images.get(anchor_vehicle_id, [])\n",
    "        pos_candidates = [i for i in pos_candidates if i != idx]\n",
    "        if len(pos_candidates) > 0:\n",
    "            pos_idx = random.choice(pos_candidates)\n",
    "        else:\n",
    "            pos_idx = idx\n",
    "\n",
    "        neg_vehicle_id = random.choice([vid for vid in self.vehicle_ids if vid != anchor_vehicle_id])\n",
    "        neg_idx = random.choice(self.vehicle_to_images[neg_vehicle_id])\n",
    "\n",
    "        try:\n",
    "            anchor_img = Image.open(anchor_item['img_path']).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading anchor image {anchor_item['img_path']}: {e}\")\n",
    "            anchor_img = Image.new('RGB', (224, 224), color=(0, 0, 0))\n",
    "        try:\n",
    "            pos_img = Image.open(self.data[pos_idx]['img_path']).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading positive image {self.data[pos_idx]['img_path']}: {e}\")\n",
    "            pos_img = Image.new('RGB', (224, 224), color=(0, 0, 0))\n",
    "        try:\n",
    "            neg_img = Image.open(self.data[neg_idx]['img_path']).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading negative image {self.data[neg_idx]['img_path']}: {e}\")\n",
    "            neg_img = Image.new('RGB', (224, 224), color=(0, 0, 0))\n",
    "\n",
    "        if self.transform:\n",
    "            anchor_img = self.transform(anchor_img)\n",
    "            pos_img = self.transform(pos_img)\n",
    "            neg_img = self.transform(neg_img)\n",
    "\n",
    "        anchor_vehicle_idx = self.vehicle_id_to_idx[anchor_vehicle_id]\n",
    "        anchor_camera_idx = self.camera_id_to_idx[anchor_item['camera_id']]\n",
    "\n",
    "        pos_vehicle_id_raw = self.data[pos_idx]['vehicle_id']\n",
    "        pos_camera_id_raw = self.data[pos_idx]['camera_id']\n",
    "        pos_vehicle_idx = self.vehicle_id_to_idx[pos_vehicle_id_raw]\n",
    "        pos_camera_idx = self.camera_id_to_idx[pos_camera_id_raw]\n",
    "\n",
    "        neg_vehicle_id_raw = self.data[neg_idx]['vehicle_id']\n",
    "        neg_camera_id_raw = self.data[neg_idx]['camera_id']\n",
    "        neg_vehicle_idx = self.vehicle_id_to_idx[neg_vehicle_id_raw]\n",
    "        neg_camera_idx = self.camera_id_to_idx[neg_camera_id_raw]\n",
    "\n",
    "        return {\n",
    "            'anchor': anchor_img,\n",
    "            'positive': pos_img,\n",
    "            'negative': neg_img,\n",
    "            'anchor_vehicle_id': anchor_vehicle_idx,\n",
    "            'anchor_camera_id_idx': anchor_camera_idx,\n",
    "            'positive_vehicle_id': pos_vehicle_idx,\n",
    "            'positive_camera_id_idx': pos_camera_idx,\n",
    "            'negative_vehicle_id': neg_vehicle_idx,\n",
    "            'negative_camera_id_idx': neg_camera_idx,\n",
    "        }\n",
    "\n",
    "class PartBasedModel(nn.Module):\n",
    "    def __init__(self, backbone, num_parts=6, feature_dim=2048):\n",
    "        super(PartBasedModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.num_parts = num_parts\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.part_pool = nn.AdaptiveAvgPool2d((num_parts, 1))\n",
    "        self.part_bn = nn.BatchNorm1d(feature_dim * num_parts)\n",
    "        self.part_fc = nn.Linear(feature_dim * num_parts, feature_dim)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.global_bn = nn.BatchNorm1d(feature_dim)\n",
    "        self.global_fc = nn.Linear(feature_dim, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone.conv1(x)\n",
    "        features = self.backbone.bn1(features)\n",
    "        features = self.backbone.relu(features)\n",
    "        features = self.backbone.maxpool(features)\n",
    "        features = self.backbone.layer1(features)\n",
    "        features = self.backbone.layer2(features)\n",
    "        features = self.backbone.layer3(features)\n",
    "        features = self.backbone.layer4(features)\n",
    "\n",
    "        global_feat = self.global_pool(features).flatten(1)\n",
    "        global_feat = self.global_bn(global_feat)\n",
    "        global_feat = self.global_fc(global_feat)\n",
    "\n",
    "        part_feat = self.part_pool(features)\n",
    "        part_feat = part_feat.flatten(1)\n",
    "        part_feat = self.part_bn(part_feat)\n",
    "        part_feat = self.part_fc(part_feat)\n",
    "\n",
    "        combined_feat = global_feat + part_feat\n",
    "        combined_feat = F.normalize(combined_feat, p=2, dim=1)\n",
    "\n",
    "        return combined_feat, global_feat, part_feat\n",
    "\n",
    "class VehicleReIDModel(nn.Module):\n",
    "    def __init__(self, num_vehicles, feature_dim=2048, num_parts=6):\n",
    "        super(VehicleReIDModel, self).__init__()\n",
    "        backbone = models.resnet50(pretrained=True)\n",
    "        self.part_model = PartBasedModel(backbone, num_parts, feature_dim)\n",
    "        self.classifier = nn.Linear(feature_dim, num_vehicles)\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        combined_feat, global_feat, part_feat = self.part_model(x)\n",
    "        if self.training:\n",
    "            logits = self.classifier(combined_feat)\n",
    "            return combined_feat, logits\n",
    "        else:\n",
    "            return combined_feat\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.3):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        pos_dist = F.pairwise_distance(anchor, positive, p=2)\n",
    "        neg_dist = F.pairwise_distance(anchor, negative, p=2)\n",
    "        loss = F.relu(pos_dist - neg_dist + self.margin)\n",
    "        return loss.mean()\n",
    "\n",
    "class SpatioTemporalFilter:\n",
    "    def __init__(self, camera_distances_file=None):\n",
    "        self.camera_distances = None\n",
    "        if camera_distances_file:\n",
    "            self._load_camera_distances(camera_distances_file)\n",
    "\n",
    "    def _load_camera_distances(self, file_path):\n",
    "        \"\"\"Load camera distances from camera_Dist.txt into a numpy array.\"\"\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        self.camera_distances = np.array([list(map(float, line.strip().split())) for line in lines])\n",
    "\n",
    "    def filter_results(self, query_camera_idx, gallery_camera_idxs, distances):\n",
    "        \"\"\"Filter distances based on camera indices, penalizing same-camera matches.\"\"\"\n",
    "        filtered_distances = distances.copy()\n",
    "        for i, gallery_cam_idx in enumerate(gallery_camera_idxs):\n",
    "            if query_camera_idx == gallery_cam_idx:\n",
    "                filtered_distances[i] *= 1.5  # Penalize same-camera matches\n",
    "        return filtered_distances\n",
    "\n",
    "class GraphReRanking:\n",
    "    def __init__(self, k1=20, k2=6, lambda_value=0.3):\n",
    "        self.k1 = k1\n",
    "        self.k2 = k2\n",
    "        self.lambda_value = lambda_value\n",
    "\n",
    "    def re_rank(self, query_features, gallery_features):\n",
    "        \"\"\"Re-rank the gallery images based on initial distance matrix.\"\"\"\n",
    "        dist_matrix = self._compute_distance_matrix(query_features, gallery_features)\n",
    "        re_ranked_dist = self._k_reciprocal_rerank(dist_matrix)\n",
    "        return re_ranked_dist\n",
    "\n",
    "    def _compute_distance_matrix(self, query_feat, gallery_feat):\n",
    "        q_feat = F.normalize(query_feat, p=2, dim=1)\n",
    "        g_feat = F.normalize(gallery_feat, p=2, dim=1)\n",
    "        dist_matrix = torch.cdist(q_feat, g_feat, p=2)\n",
    "        return dist_matrix.cpu().numpy()\n",
    "\n",
    "    def _k_reciprocal_rerank(self, dist_matrix):\n",
    "        # Placeholder: Currently returns original distances\n",
    "        # Future implementation could use k-reciprocal encoding\n",
    "        return dist_matrix\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    @staticmethod\n",
    "    def compute_mAP(query_labels, gallery_labels, query_cameras, gallery_cameras, dist_matrix):\n",
    "        \"\"\"Compute mean Average Precision using a precomputed distance matrix.\"\"\"\n",
    "        mAPs = []\n",
    "        for i in range(len(query_labels)):\n",
    "            query_label = query_labels[i]\n",
    "            query_camera = query_cameras[i]\n",
    "            distances = dist_matrix[i]\n",
    "            gt_matches = (gallery_labels == query_label) & (gallery_cameras != query_camera)\n",
    "            if gt_matches.sum() == 0:\n",
    "                continue\n",
    "            sorted_indices = np.argsort(distances)\n",
    "            sorted_gt = gt_matches[sorted_indices]\n",
    "            ap = average_precision_score(sorted_gt, -distances[sorted_indices])\n",
    "            mAPs.append(ap)\n",
    "        return np.mean(mAPs) if mAPs else 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_CMC(query_labels, gallery_labels, query_cameras, gallery_cameras, dist_matrix, ranks=[1, 5, 10]):\n",
    "        \"\"\"Compute CMC scores using a precomputed distance matrix.\"\"\"\n",
    "        cmc_scores = {rank: 0 for rank in ranks}\n",
    "        valid_queries = 0\n",
    "        for i in range(len(query_labels)):\n",
    "            query_label = query_labels[i]\n",
    "            query_camera = query_cameras[i]\n",
    "            distances = dist_matrix[i]\n",
    "            gt_matches = (gallery_labels == query_label) & (gallery_cameras != query_camera)\n",
    "            if gt_matches.sum() == 0:\n",
    "                continue\n",
    "            valid_queries += 1\n",
    "            sorted_indices = np.argsort(distances)\n",
    "            sorted_gt = gt_matches[sorted_indices]\n",
    "            for rank in ranks:\n",
    "                if sorted_gt[:rank].sum() > 0:\n",
    "                    cmc_scores[rank] += 1\n",
    "        for rank in ranks:\n",
    "            cmc_scores[rank] /= valid_queries if valid_queries > 0 else 1\n",
    "        return cmc_scores\n",
    "\n",
    "def get_transforms(is_training=True):\n",
    "    if is_training:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.RandomCrop((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3))\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=100):\n",
    "    \"\"\"Training loop with triplet loss and classification loss.\"\"\"\n",
    "    triplet_loss_fn = TripletLoss(margin=0.3)\n",
    "    ce_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            if 'anchor' in batch:\n",
    "                anchor = batch['anchor'].to(device)\n",
    "                positive = batch['positive'].to(device)\n",
    "                negative = batch['negative'].to(device)\n",
    "\n",
    "                anchor_feat, _ = model(anchor)\n",
    "                pos_feat, _ = model(positive)\n",
    "                neg_feat, _ = model(negative)\n",
    "\n",
    "                loss = triplet_loss_fn(anchor_feat, pos_feat, neg_feat)\n",
    "            else:\n",
    "                images = batch['img'].to(device)\n",
    "                labels = batch['vehicle_id']\n",
    "                if isinstance(labels, torch.Tensor):\n",
    "                    labels = labels.to(device)\n",
    "                else:\n",
    "                    labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "\n",
    "                features, logits = model(images)\n",
    "                loss = ce_loss_fn(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_loader) if len(train_loader) > 0 else 0.0\n",
    "        print(f'Epoch {epoch} completed, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "def evaluate_model(model, query_loader, gallery_loader, device, data_dir):\n",
    "    \"\"\"Evaluate the model using mAP and CMC metrics with spatio-temporal filtering and re-ranking.\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    query_features, query_labels = [], []\n",
    "    gallery_features, gallery_labels = [], []\n",
    "    query_cameras, gallery_cameras = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in query_loader:\n",
    "            images = batch['img'].to(device)\n",
    "            feats = model(images)\n",
    "            feats = feats.cpu()\n",
    "            query_features.append(feats)\n",
    "            query_labels.extend(batch['vehicle_id'].cpu().tolist() if isinstance(batch['vehicle_id'], torch.Tensor) else batch['vehicle_id'])\n",
    "            query_cameras.extend(batch['camera_id_idx'].cpu().tolist() if isinstance(batch['camera_id_idx'], torch.Tensor) else batch['camera_id_idx'])\n",
    "\n",
    "        for batch in gallery_loader:\n",
    "            images = batch['img'].to(device)\n",
    "            feats = model(images)\n",
    "            feats = feats.cpu()\n",
    "            gallery_features.append(feats)\n",
    "            gallery_labels.extend(batch['vehicle_id'].cpu().tolist() if isinstance(batch['vehicle_id'], torch.Tensor) else batch['vehicle_id'])\n",
    "            gallery_cameras.extend(batch['camera_id_idx'].cpu().tolist() if isinstance(batch['camera_id_idx'], torch.Tensor) else batch['camera_id_idx'])\n",
    "\n",
    "    if len(query_features) > 0:\n",
    "        query_features = torch.cat(query_features, dim=0)\n",
    "    else:\n",
    "        query_features = torch.empty((0, model.feature_dim))\n",
    "\n",
    "    if len(gallery_features) > 0:\n",
    "        gallery_features = torch.cat(gallery_features, dim=0)\n",
    "    else:\n",
    "        gallery_features = torch.empty((0, model.feature_dim))\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    query_labels_np = np.array(query_labels)\n",
    "    gallery_labels_np = np.array(gallery_labels)\n",
    "    query_cameras_np = np.array(query_cameras)\n",
    "    gallery_cameras_np = np.array(gallery_cameras)\n",
    "\n",
    "    # Initialize reranker and spatio-temporal filter\n",
    "    reranker = GraphReRanking()\n",
    "    st_filter = SpatioTemporalFilter(camera_distances_file=os.path.join(data_dir, 'camera_Dist.txt'))\n",
    "\n",
    "    # Compute re-ranked distance matrix\n",
    "    re_ranked_dist = reranker.re_rank(query_features, gallery_features)\n",
    "\n",
    "    # Apply spatio-temporal filtering\n",
    "    filtered_dist_matrix = np.zeros_like(re_ranked_dist)\n",
    "    for i in range(len(query_cameras_np)):\n",
    "        query_camera_idx = query_cameras_np[i]\n",
    "        filtered_dist_matrix[i] = st_filter.filter_results(query_camera_idx, gallery_cameras_np, re_ranked_dist[i])\n",
    "\n",
    "    # Compute metrics\n",
    "    mAP = EvaluationMetrics.compute_mAP(\n",
    "        query_labels_np, gallery_labels_np, query_cameras_np, gallery_cameras_np, filtered_dist_matrix\n",
    "    )\n",
    "    cmc_scores = EvaluationMetrics.compute_CMC(\n",
    "        query_labels_np, gallery_labels_np, query_cameras_np, gallery_cameras_np, filtered_dist_matrix\n",
    "    )\n",
    "\n",
    "    print(f'mAP: {mAP:.4f}')\n",
    "    print(f'CMC Rank-1: {cmc_scores.get(1, 0):.4f}')\n",
    "    print(f'CMC Rank-5: {cmc_scores.get(5, 0):.4f}')\n",
    "    print(f'CMC Rank-10: {cmc_scores.get(10, 0):.4f}')\n",
    "\n",
    "    return mAP, cmc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b5e79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "data_dir = '/kaggle/input/veri-vehicle-re-identification-dataset/VeRi'\n",
    "batch_size = 48\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Data transforms\n",
    "train_transform = get_transforms(is_training=True)\n",
    "test_transform = get_transforms(is_training=False)\n",
    "\n",
    "# Datasets\n",
    "train_dataset = VeRiDataset(data_dir, split='train', transform=train_transform,\n",
    "                           triplet_sampling=True)\n",
    "query_dataset = VeRiDataset(data_dir, split='query', transform=test_transform)\n",
    "test_dataset = VeRiDataset(data_dir, split='test', transform=test_transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "query_loader = DataLoader(query_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Get number of unique vehicles for classification\n",
    "all_vehicle_ids = set()\n",
    "for item in train_dataset.data:\n",
    "    all_vehicle_ids.add(item['vehicle_id'])\n",
    "num_vehicles = len(all_vehicle_ids)\n",
    "\n",
    "# Model\n",
    "model = VehicleReIDModel(num_vehicles=num_vehicles, feature_dim=2048, num_parts=6)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Query samples: {len(query_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Number of vehicles: {num_vehicles}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "print(\"Starting training...\")\n",
    "train_model(model, train_loader, query_loader, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b55428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "print(\"Starting evaluation...\")\n",
    "mAP, cmc_scores = evaluate_model(model, query_loader, test_loader, device, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a890bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), 'veri_reid_model.pth')\n",
    "print(\"Model saved as 'veri_reid_model.pth'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
